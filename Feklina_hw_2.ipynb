{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим данные (к сожалению, я взяла только 5000 отзывов, потому что на обработку большего количества не было времени). Категория Grocery and Gourme Food, данные можно скачать по ссылке https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Grocery_and_Gourmet_Food_5.json.gz"
      ],
      "metadata": {
        "id": "6CH4pzl0tBAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gntZD7PYB644",
        "outputId": "6e053469-306a-4457-869d-c8cbfb9b8967"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"drive/MyDrive/avtobr/food_data.json.gz\"\n",
        "my_folder = \"drive/MyDrive/avtobr/\""
      ],
      "metadata": {
        "id": "iar4OxVJCfY8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "def data_parse(path):\n",
        "    js_data = []\n",
        "    g = gzip.open(path, 'rb')\n",
        "    for l in g:\n",
        "        js_data.append(json.loads(l))\n",
        "    return js_data\n",
        "\n",
        "data = data_parse(data_path)"
      ],
      "metadata": {
        "id": "dHpsM-CaFu-F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_big = data"
      ],
      "metadata": {
        "id": "ulDEfdPY5gal"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[:5000]"
      ],
      "metadata": {
        "id": "palHmnVbuj8-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data[150:155]:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "c1AgJtN7F7I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Методы выделения упоминаний:\n",
        "\n",
        "1) через wordnet: берем гипонимы слов \"еда\", \"напиток\" и др., потом можно взять гипонимы гипонимов (или даже сделать несколько таких итераций) и получить список разной возможной еды. Это быстро, но скорее всего мы так потеряем большую часть вариантов, потому что еду обычно называют по типу продукта (чай, паста, салат и т.д.), и наименований слишком много, чтобы этот метод достал все\n",
        "\n",
        "2) похожий способ, но через word2vec: обучить модель на корпусе отзывов и найти самые похожие слова через близость к слову \"еда\" и похожим.\n",
        "\n",
        "3) Можно сделать шаблоны вроде \"this NOUN\", так как кажется в отзывах люди часто упоминают название объекта именно в контекстах вроде \"этот чай очень вкусный\". Также это можно комбинировать с методом w2v, чтобы отфильтровать слова, попавшие туда по ошибке\n",
        "\n",
        "\n",
        "Реализовывать буду вариант 3"
      ],
      "metadata": {
        "id": "LZckD1QfDkuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "\n",
        "import re\n",
        "import stanza\n",
        "from tqdm import tqdm\n",
        "stanza.download('en')\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,lemma,pos', tokenize_no_ssplit=True)"
      ],
      "metadata": {
        "id": "7amo4ATHP6df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Для начала найдём названия: всё, что стоит после \"this is|this|these\", при этом дальше идёт проверка части речи, и перед существительным может идти сколько угодно артиклей и прилагательных, это поможет найт больше названий."
      ],
      "metadata": {
        "id": "aykY31_9xlq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "\n",
        "for review in tqdm(data):\n",
        "    if 'reviewText' in review:\n",
        "        rev_text = review['reviewText']\n",
        "        reg_res = [res[1] for res in re.findall(r'(this is|this|these) ([\\w\\s]+)', rev_text)]\n",
        "        for res in reg_res:\n",
        "            group = [(word.text, word.upos) for sent in nlp(res).sentences for word in sent.words]\n",
        "            for word in group:\n",
        "                noun_phrase = []\n",
        "                if word[1] == 'ADJ' or word[1] == 'DET':\n",
        "                    continue\n",
        "                elif word[1] == 'NOUN':\n",
        "                    entities.append(word[0])\n",
        "                else:\n",
        "                    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZtkq0NQSXjn",
        "outputId": "cf75861e-5eff-4567-e641-a038d2441cdc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [06:36<00:00, 12.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ent_list = list(set(entities))"
      ],
      "metadata": {
        "id": "7--eJxUXX7DN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent_list = [el.lower() for el in ent_list]"
      ],
      "metadata": {
        "id": "U1teHtWt5wnR"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "теперь подключим w2v, попробуем удалить несовпадающие"
      ],
      "metadata": {
        "id": "6VAZiCgQBIgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import logging\n",
        "import urllib.request\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "21uioFyC7jfa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [review['reviewText'] for review in data if 'reviewText' in review]"
      ],
      "metadata": {
        "id": "bohI280nuqrj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(my_folder + 'ent_list.txt', 'r', encoding='utf-8') as f:\n",
        "    ent_list = list(f.read()[2:-2].split('\\', \\''))"
      ],
      "metadata": {
        "id": "5__57fX135JF"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(my_folder + 'reviews.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(reviews))"
      ],
      "metadata": {
        "id": "0cPJxWBuzutP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(my_folder + 'ent_list.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(str(ent_list))"
      ],
      "metadata": {
        "id": "yyboOVZv0zIU"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_data = gensim.models.word2vec.LineSentence(my_folder + 'reviews.txt')"
      ],
      "metadata": {
        "id": "oH-HchWcz4QP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_food = gensim.models.Word2Vec(w2v_data, size=300, window=5, min_count=1)"
      ],
      "metadata": {
        "id": "0SK7WzEH_Zv9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ent_list.remove(antimatch)\n",
        "antimatch = model_food.wv.doesnt_match(ent_list)\n",
        "print(antimatch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNj9yMyysWft",
        "outputId": "5775759d-a83b-45ae-a076-48a1d0195b66"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:vectors for words {'field', 'deliciousness', 'shortcut', 'reformulation', 'organs', 'merchandise', 'dwindles', 'originals', 'deception', 'stage', 'faithful', 'colorant', 'mousses', 'Treats', 'ointment'} are not present in the model, ignoring these words\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quinoa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получилось довольно плохо, в лишних довольно быстро начали появляться названия продуктов, поэтому в результате отфильтровать получилось плохо.\n",
        "\n",
        "Теперь найдём все биграммы с этими словами"
      ],
      "metadata": {
        "id": "z1QN6h_DGVQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "left_context = []\n",
        "right_context = []\n",
        "\n",
        "for review in reviews:\n",
        "    review = review.translate(str.maketrans('', '', punctuation)).lower()\n",
        "    review = review.split()\n",
        "    for i, word in enumerate(review):\n",
        "        if word in ent_list:\n",
        "            if i > 0:\n",
        "                left_context.extend([review[i - 1], review[i], '|'])\n",
        "            if i + 1 < len(review):\n",
        "                right_context.extend([review[i], review[i + 1], '|'])"
      ],
      "metadata": {
        "id": "J8dm5sXut8rM"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "ignored_words = nltk.corpus.stopwords.words('english') + ['|']\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()"
      ],
      "metadata": {
        "id": "UWcO-YsA8uEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью nltk посмотрим, как выделяют коллокации три различные метрики:\n",
        "\n",
        "(стоп-слова, артикли и т.д. удаляются на этом этапе)"
      ],
      "metadata": {
        "id": "Nx778ebOGqAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_finder = BigramCollocationFinder.from_words(left_context)\n",
        "l_finder.apply_word_filter(lambda w: w.lower() in ignored_words)\n",
        "\n",
        "print('pmi:', *l_finder.nbest(bigram_measures.pmi, 20))\n",
        "print('student_t:', *l_finder.nbest(bigram_measures.student_t, 20))\n",
        "print('raw_freq:', *l_finder.nbest(bigram_measures.raw_freq, 20))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "r_finder = BigramCollocationFinder.from_words(right_context)\n",
        "r_finder.apply_word_filter(lambda w: w.lower() in ignored_words)\n",
        "print('pmi:', *r_finder.nbest(bigram_measures.pmi, 20))\n",
        "print('student_t:', *r_finder.nbest(bigram_measures.student_t, 20))\n",
        "print('raw_freq:', *r_finder.nbest(bigram_measures.raw_freq, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldvk7lKf82_m",
        "outputId": "7d745ab2-feef-400f-9d4a-63d226ada1cc"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi: [('caviar', 'sourkrout'), ('mortal', 'coil'), ('splendid', 'potion'), ('choking', 'stage'), ('discovered', 'aidells'), ('dingy', 'colours'), ('index', 'rates'), ('teasthis', 'wording'), ('pay', '6box'), ('british', 'confections'), ('flagitious', 'concoction'), ('trying', 'quinoa'), ('chronic', 'allergies'), ('gumball', 'category'), ('reviewer', 'ointment'), ('rockthese', 'meltaways'), ('web', 'page'), ('british', 'delights'), ('childhood', 'beverage'), ('evening', 'beverage')]\n",
            "student_t: [('hot', 'sauce'), ('great', 'product'), ('hot', 'sauces'), ('black', 'tea'), ('vanilla', 'extract'), ('iced', 'tea'), ('ghost', 'pepper'), ('great', 'price'), ('good', 'price'), ('good', 'product'), ('great', 'flavor'), ('good', 'flavor'), ('celestial', 'seasonings'), ('good', 'quality'), ('great', 'taste'), ('excellent', 'product'), ('green', 'curry'), ('corn', 'syrup'), ('green', 'tea'), ('davids', 'cookies')]\n",
            "raw_freq: [('hot', 'sauce'), ('great', 'product'), ('hot', 'sauces'), ('black', 'tea'), ('great', 'flavor'), ('vanilla', 'extract'), ('good', 'flavor'), ('great', 'price'), ('iced', 'tea'), ('good', 'product'), ('ghost', 'pepper'), ('great', 'taste'), ('good', 'price'), ('good', 'quality'), ('celestial', 'seasonings'), ('excellent', 'product'), ('corn', 'syrup'), ('green', 'curry'), ('green', 'tea'), ('chai', 'tea')]\n",
            "\n",
            "\n",
            "pmi: [('deception', 'corruption'), ('skulls', 'floating'), ('disaster', 'soif'), ('aidells', 'cajun'), ('colours', 'supposing'), ('article', 'httppreventdiseasecomnews12072312newresearchcastsdoubtonpasteurizationtechniquesformilkshtmlutmsource072312utmcampaign072312utmmediumemail'), ('deliciousness', 'buy'), ('fajita', 'grill'), ('bubblemint', '14count'), ('companys', 'address'), ('delights', 'choppedup'), ('picante', 'heads'), ('reformulation', 'update'), ('crap', 'niether'), ('requirement', 'oknonsense'), ('requirement', 'sheesh'), ('cheesecake', 'factory'), ('aidells', 'chicken'), ('car', 'somehow'), ('creamers', 'pull')]\n",
            "student_t: [('grocery', 'store'), ('tastes', 'like'), ('vanilla', 'extract'), ('ghost', 'pepper'), ('taste', 'like'), ('tastes', 'great'), ('sugar', 'free'), ('corn', 'syrup'), ('taste', 'great'), ('curry', 'paste'), ('years', 'ago'), ('tastes', 'good'), ('chai', 'tea'), ('jelly', 'belly'), ('works', 'great'), ('chocolate', 'chip'), ('jelly', 'beans'), ('vanilla', 'beans'), ('coconut', 'milk'), ('taste', 'good')]\n",
            "raw_freq: [('grocery', 'store'), ('tastes', 'like'), ('vanilla', 'extract'), ('ghost', 'pepper'), ('taste', 'like'), ('tastes', 'great'), ('sugar', 'free'), ('taste', 'great'), ('corn', 'syrup'), ('chai', 'tea'), ('curry', 'paste'), ('tastes', 'good'), ('years', 'ago'), ('jelly', 'belly'), ('works', 'great'), ('chocolate', 'chip'), ('vanilla', 'beans'), ('jelly', 'beans'), ('coconut', 'milk'), ('taste', 'good')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мне показалось, что правый контекст как правило не очень информативен, а pmi выделяет много редких (зачастую случайных) сочетаний, поэтому возьмём t-score:"
      ],
      "metadata": {
        "id": "yyIQTFKkG37Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in ['tea', 'sauce', 'product', 'drink', 'stuff']:\n",
        "    ent_left_context = []\n",
        "    for review in reviews:\n",
        "        review = review.translate(str.maketrans('', '', punctuation)).lower()\n",
        "        review = review.replace('the ', '').replace(' a ', ' ')\n",
        "        review = review.split()\n",
        "        for i, word in enumerate(review):\n",
        "            if word == entity:\n",
        "                if i > 0:\n",
        "                    ent_left_context.extend([review[i - 1], review[i], '|'])\n",
        "    finder = BigramCollocationFinder.from_words(ent_left_context)\n",
        "    finder.apply_word_filter(lambda w: w.lower() in ignored_words)\n",
        "    print(entity, 'student_t:', *finder.nbest(bigram_measures.student_t, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrhEx3IM_fKw",
        "outputId": "6bebc42b-304a-4a32-98be-a803d791c1f1"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tea student_t: ('black', 'tea') ('iced', 'tea') ('green', 'tea') ('chai', 'tea') ('good', 'tea') ('hot', 'tea') ('ice', 'tea') ('lipton', 'tea') ('peach', 'tea') ('favorite', 'tea')\n",
            "sauce student_t: ('hot', 'sauce') ('soy', 'sauce') ('insanity', 'sauce') ('pepper', 'sauce') ('seasoning', 'sauce') ('bbq', 'sauce') ('hottest', 'sauce') ('chili', 'sauce') ('death', 'sauce') ('wing', 'sauce')\n",
            "product student_t: ('great', 'product') ('good', 'product') ('excellent', 'product') ('quality', 'product') ('awesome', 'product') ('nice', 'product') ('ok', 'product') ('organic', 'product') ('vanilla', 'product') ('wonderful', 'product')\n",
            "drink student_t: ('dont', 'drink') ('cold', 'drink') ('cant', 'drink') ('morning', 'drink') ('rather', 'drink') ('delicious', 'drink') ('favorite', 'drink') ('never', 'drink') ('rarely', 'drink') ('bedtime', 'drink')\n",
            "stuff student_t: ('good', 'stuff') ('great', 'stuff') ('hot', 'stuff') ('best', 'stuff') ('cheap', 'stuff') ('like', 'stuff') ('real', 'stuff') ('spicy', 'stuff') ('tasting', 'stuff') ('amazing', 'stuff')\n"
          ]
        }
      ]
    }
  ]
}
