{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab3c473",
   "metadata": {},
   "source": [
    "Функция, лемматизирующая текст (и изначально очищала от стоп-слов, но так не работал один из методов позднее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee146be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "PUNCT = punctuation + '-'\n",
    "morph = MorphAnalyzer()\n",
    "SW = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f16cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(my_line):\n",
    "    my_line = my_line.translate(str.maketrans('', '', PUNCT))\n",
    "    words = my_line.split()\n",
    "    lem_words = [morph.parse(w.replace('‘', '').replace('’', ''))[0].normal_form for w in words]\n",
    "    #filtered = [w for w in lem_words if w not in SW]\n",
    "    return lem_words #filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba9d7d",
   "metadata": {},
   "source": [
    "В качестве корпуса используем сайт школы лингвистики, раздел с новостями (рубрика \"наука\", т.к. там обычно больше тегов стоит). Автоматически собираем ссылки, заголовки, текст новости и теги. Для каждой новости создаём словарь, где хранятся все данные, связанные с ней (туда же будем сохранять автоматические ключевые слова разных методов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f62cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a3077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 0\n",
    "texts = 0\n",
    "data = []\n",
    "\n",
    "req = urllib.request.Request('https://ling.hse.ru/news/science/', headers = {'User-Agent':user_agent})\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    html = response.read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "news = soup.find_all('div', {'class': \"post__content\"})\n",
    "tag_set = soup.find_all('div', {'class': \"tag-set\"})\n",
    "for i in range(len(news)):\n",
    "    \n",
    "    if tokens > 5000:\n",
    "        break\n",
    "    \n",
    "    news_link = news[i].find('a').get('href')\n",
    "    news_title = news[i].find('a').get('title')\n",
    "    tags = [tag.get('title') for tag in tag_set[i+1].find_all('a', {'class': \"tag rubric rubric--tag\"})]\n",
    "    \n",
    "    if 'ling' in news_link:# новости с других источников не парсятся так же\n",
    "        req = urllib.request.Request(news_link, headers = {'User-Agent':user_agent})\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            html = response.read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        text = soup.find('div', {'class': \"post__text\"}).text\n",
    "        lemmas_text = lemmatize(news_title + text)\n",
    "        \n",
    "        news_dict = dict()\n",
    "        news_dict['link'] = news_link\n",
    "        news_dict['title'] = news_title\n",
    "        news_dict['auto_tags'] = tags\n",
    "        news_dict['text'] = text\n",
    "        news_dict['lemmas'] = lemmas_text\n",
    "        \n",
    "        data.append(news_dict)\n",
    "        tokens += len(lemmas_text)\n",
    "        texts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffb65d",
   "metadata": {},
   "source": [
    "Проверяем размер корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f922e033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 текстов, 4285 токенов\n"
     ]
    }
   ],
   "source": [
    "print(texts, 'текстов,', tokens, 'токенов')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9185f93",
   "metadata": {},
   "source": [
    "Добавляем размеченные вручную ключевые слова. Проблема тегов на сайте шл в том, что не все из этих слов реально есть в тексте, поэтому эталоном станет объединение ключевых слов, выделенных вручную, и тех тегов, в которых все слова тега есть в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768b8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = ['западнодвинская экспедиция, диалектологическая экспедиция, тверская область, местные жители, деревня, шетнево, макеево, разговор, лексика, студенты',\n",
    " 'honorable mention, Нияз Киреев, студент, премия, конференция, Мемориальная премия имени Чарльза Таунсэнда',\n",
    " 'доклад, иранские языки, конференция, Институт лингвистики РГГУ',\n",
    " 'Иван Саркисов, диссертация, защита, тайская и бирманская поэзия',\n",
    " 'Ольга Драгой, Анна Журавлева, центр языка и мозга, грант, аспирант, картирование речи, конкурс, Научный центр \"Идея\"',\n",
    " 'фольклорная экспедиция, костромская область, Большие Рымы, Макарьевский район, унжлаг, традиции, жители, валенки, обряды, стихи, верования, исследование',\n",
    " 'НУГ, иранские языки, конференция, доклад',\n",
    " 'Нияз Киреев, студент, конференция, SLS Annual Meetings, доклад, славистика, пленарный доклад, Фридман, фонетика, Зализняк'\n",
    "]\n",
    "iter_tags = iter(my_tags)\n",
    "\n",
    "for new in data:\n",
    "    new['my_tags'] = next(iter_tags).split(', ')\n",
    "    my_tags_lemmas = [tuple(lemmatize(w)) for w in new['my_tags']]\n",
    "    auto_tags_lemmas = [tuple(lemmatize(w)) for w in new['auto_tags']]\n",
    "    text_words = []\n",
    "    for tag in auto_tags_lemmas:\n",
    "        s = 0\n",
    "        for word in tag:\n",
    "            if word in new['lemmas']:\n",
    "                s += 1\n",
    "        if s == len(tag):\n",
    "            text_words.append(tag)\n",
    "    new['key_words'] = list(set(my_tags_lemmas).union(set(text_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b78572",
   "metadata": {},
   "source": [
    "Методами автоматического выделения будут RAKE, TextRank и Tf-Idf. Устанавливаем всё необходимое и делаем матрицу tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87dba559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install python-rake\n",
    "#!pip3 install summa\n",
    "import RAKE\n",
    "rake = RAKE.Rake(SW)\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7301778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "corpus = []\n",
    "for new in data:\n",
    "    corpus.append(' '.join(new['lemmas']))\n",
    "\n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb14c21",
   "metadata": {},
   "source": [
    "Добавляем результаты автоматического выделения в словари и одновременно с этим создаём фильтрованные варианты. Я заметила, что в ключевых словах часто оказываются, например, глаголы, причём не самые содержательные, поэтому в качестве фильтра оставим только ключевые слова, состоящие из существительного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa82a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matr_iter = iter(matrix)\n",
    "\n",
    "for new in data:\n",
    "    text = ' '.join(new['lemmas'])\n",
    "    \n",
    "    rake_words = rake.run(text, maxWords=2, minFrequency=1)[:15]\n",
    "    new['rake'] = [tuple(key_word[0].split()) for key_word in rake_words]\n",
    "    new['rake_filtered'] = [key_word for key_word in new['rake'] if len(key_word) == 1 and morph.parse(key_word[0])[0].tag.POS == 'NOUN']\n",
    "    \n",
    "    textrank_words = keywords.keywords(text, language='russian', additional_stopwords=SW, scores=True)[:15]\n",
    "    new['textrank'] = [tuple(key_word[0].split()) for key_word in textrank_words]\n",
    "    new['textrank_filtered'] = [key_word for key_word in new['textrank'] if len(key_word) == 1 and morph.parse(key_word[0])[0].tag.POS == 'NOUN']\n",
    "    \n",
    "    tfidf_words = [x for _, x in sorted(zip(next(matr_iter).toarray()[0], terms), reverse=True) if x not in SW][:15]\n",
    "    new['tf-idf'] = [tuple(key_word.split()) for key_word in tfidf_words]\n",
    "    new['tf-idf_filtered'] = [key_word for key_word in new['tf-idf'] if len(key_word) == 1 and morph.parse(key_word[0])[0].tag.POS == 'NOUN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045673a8",
   "metadata": {},
   "source": [
    "Считаем precision и recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206c4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for new in data:\n",
    "    for method in ['rake', 'textrank', 'tf-idf']:\n",
    "        intersection = len(set(new['key_words']).intersection(set(new[method])))\n",
    "        filt_intersection = len(set(new['key_words']).intersection(set(new[method + '_filtered'])))\n",
    "    \n",
    "        if intersection == 0:\n",
    "            new[method + '_precision'] = 0\n",
    "            new[method + '_filt_precision'] = 0\n",
    "            new[method + '_recall'] = 0\n",
    "            new[method + '_filt_recall'] = 0\n",
    "        elif filt_intersection == 0:\n",
    "            new[method + '_precision'] = intersection/len(new[method])\n",
    "            new[method + '_filt_precision'] = 0\n",
    "            new[method + '_recall'] = intersection/len(new['key_words'])\n",
    "            new[method + '_filt_recall'] = 0\n",
    "        else:\n",
    "            new[method + '_precision'] = intersection/len(new[method])\n",
    "            new[method + '_filt_precision'] = filt_intersection/len(new[method + '_filtered'])\n",
    "            new[method + '_recall'] = intersection/len(new['key_words'])\n",
    "            new[method + '_filt_recall'] = filt_intersection/len(new['key_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f7bc8",
   "metadata": {},
   "source": [
    "Считаем средний показатель этих метрик по текстам и f1-score, смотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35006aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, filt. Precision; Recall, filt. Recall; F1, filt. F1\n",
      "rake: 0.09, 0.13; 0.15, 0.06; 0.11, 0.08\n",
      "textrank: 0.08, 0.19; 0.16, 0.16; 0.11, 0.17\n",
      "tf-idf: 0.15, 0.23; 0.31, 0.28; 0.2, 0.25\n"
     ]
    }
   ],
   "source": [
    "print('Precision, filt. Precision; Recall, filt. Recall; F1, filt. F1')\n",
    "for method in ['rake', 'textrank', 'tf-idf']:\n",
    "    precision = round(sum([new[method + '_precision'] for new in data])/8, 2)\n",
    "    filt_precision = round(sum([new[method + '_filt_precision'] for new in data])/8, 2)\n",
    "    recall = round(sum([new[method + '_recall'] for new in data])/8, 2)\n",
    "    filt_recall = round(sum([new[method + '_filt_recall'] for new in data])/8, 2)\n",
    "    f1score = round(2*precision*recall/(precision + recall), 2)\n",
    "    filt_f1score = round(2*filt_precision*filt_recall/(filt_precision + filt_recall), 2)\n",
    "    print(f'{method}: {precision}, {filt_precision}; {recall}, {filt_recall}; {f1score}, {filt_f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db07ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эталон:  [('разговор',), ('диалектологический', 'экспедиция'), ('лексика',), ('тверская', 'область'), ('деревня',), ('местный', 'житель'), ('студент',), ('бакалавриат',), ('шетнево',), ('макеево',), ('западнодвинский', 'экспедиция')] \n",
      "\n",
      "RAKE:  [('тверская', 'область'), ('смоленский', 'говор'), ('образ', 'исследователь'), ('точно', 'уверенный'), ('работа', 'например'), ('действительность', 'встречаться'), ('это', 'утратить'), ('вооружиться', 'опросник'), ('план', 'работа'), ('свой', 'дом'), ('проблема', 'решить'), ('находиться', 'недалеко'), ('довольно', 'необычный'), ('необычный', 'часы'), ('маша', 'замбржицкий')] \n",
      "\n",
      "TextRank:  [('экспедиция',), ('год',), ('деревня',), ('большой',), ('больший',), ('говор', 'местный', 'житель'), ('ещё',), ('сбор',), ('всё',), ('час',), ('часы',), ('говорить',), ('который',), ('нужно',), ('птица',)] \n",
      "\n",
      "TF-IDF:  [('деревня',), ('экспедиция',), ('житель',), ('год',), ('местный',), ('макеево',), ('шетнево',), ('птица',), ('прошлый',), ('ещё',), ('лексика',), ('сбор',), ('говор',), ('который',), ('всё',)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Эталон: ', data[0]['key_words'], '\\n')\n",
    "print('RAKE: ', data[0]['rake'], '\\n')\n",
    "print('TextRank: ', data[0]['textrank'], '\\n')\n",
    "print('TF-IDF: ', data[0]['tf-idf'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bf91e",
   "metadata": {},
   "source": [
    "Что можно заметить? У rake было довольно много тегов из нескольких слов (в том числе удачных, совпадающих с эталоном), поэтому при фильтрации recall сильно падает, а precision немного вырастает. При этом остальные два метода выделяют не выделяют или почти не выделяют таких ключевых фраз, поэтому при фильтрации уходит много глаголов и других частей речи, и precision заметно вырастает. В целом f1-score для фильтрованного варианта скорее выше (и даже для RAKE не сильно падает, несмотря на заметно пониженный recall). Лучше всего работает TF-IDF!\n",
    "\n",
    "Из лишнего выделяются некоторые случайные словосочетания и слова (\"большой\", \"ещё\", \"в действительности встречаются\"). Возможно, автор просто использует одну и ту же конструкцию для описания, поэтому она распознается как повторяющаяся и значимая. Это можно попробовать решить, например, более полным фильтром по структуре тега (чтобы ключевые фразы с глаголами или состоящие из одного прилагательного не попадали в итог); также можно было бы дополнить список стоп-слов, это помогло бы избавиться от совсем незначимых слов (\"всё\", \"который\")\n",
    "\n",
    "\n",
    "Не выделяются названия: название конференции может быть упомянуто всего один раз (или в разных формулировках, с аббревиатурой и без и т.п.), но оно значимо для текста. Это мог бы решить дополнительный этап, где в тексте бы выделялись именованные сущности и прибавлялись к ключевым словам (возможно, не все, а как-то отфильтрованные)\n",
    "\n",
    "Для русского есть некоторая проблема с однокоренными словами (в тексте есть \"верования\", \"вера\", \"верить\", но при этом каждое из слов встречается не так часто и как следствие не выделяется как ключевое). Может можно попробовать выделять корень (делать стемминг?) или считать расстояние Левенштейна и считать общую встречаемость для всех однокоренных слов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
